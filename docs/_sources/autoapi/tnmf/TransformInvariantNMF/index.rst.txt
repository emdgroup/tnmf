:mod:`tnmf.TransformInvariantNMF`
=================================

.. py:module:: tnmf.TransformInvariantNMF

.. autoapi-nested-parse::

   Transform-Invariant Non-Negative Matrix Factorization

   Authors: Adrian Šošić, Mathias Winkel



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   tnmf.TransformInvariantNMF.MiniBatchAlgorithm
   tnmf.TransformInvariantNMF.TransformInvariantNMF




.. class:: MiniBatchAlgorithm

   Bases: :py:obj:`enum.Enum`

   MiniBatch algorithms that can be used with :meth:`.TransformInvariantNMF.fit_minibatch`.

   .. attribute:: Cyclic_MU
      :annotation: = 4

      

   .. attribute:: ASG_MU
      :annotation: = 5

      

   .. attribute:: GSG_MU
      :annotation: = 6

      

   .. attribute:: ASAG_MU
      :annotation: = 7

      

   .. attribute:: GSAG_MU
      :annotation: = 8

      


.. class:: TransformInvariantNMF(n_atoms: int, atom_shape: Tuple[int, Ellipsis], inhibition_range: Union[int, Tuple[int, Ellipsis]] = None, backend: str = 'numpy_fft', logger: logging.Logger = None, verbose: int = 0, **kwargs)


   Transform Invariant Non-Negative Matrix Factorization.

   Finds non-negative tensors :attr:`W` (dictionary) and :attr:`H` (activations) that approximate the non-negative tensor
   :attr:`V` (samples) for a given transform operator`. # TODO: add link to TNMF model

   .. note::
       Currently, only a single transform type, corresponding to *shift invariance*, is supported and hard-coded. In contrast
       to other *generic* types of transforms, shift invariance can be efficiently achieved through convolution operations
       (or, equivalently, multiplication in Fourier domain). Therefore, shift invariance will remain hard-coded and retained
       as an optional transform type even when additional transforms become supported in future releases.

   Optimization is performed via multiplicative updates to :attr:`W` and :attr:`H`, see [1]_.
   Minibatch updates are possible via a selection of algorithms from [2]_.
   Different backend implementations (NumPy, PyTorch, with/without FFT, etc.) can be selected by the user.

   :param n_atoms: Number of elementary atoms. The shape of :attr:`W` will be ``(n_atoms, n_channels, *atom_shape)``.
   :type n_atoms: :class:`int`
   :param atom_shape: Shape of the elementary atoms. The shape of :attr:`W` will be ``(n_atoms, n_channels, *atom_shape)``.
   :type atom_shape: :class:`Tuple[int`, :class:`...]`
   :param inhibition_range: Lateral inhibition range. If set to None, the value is set to ``*atom_shape``, which ensures
                            that activations are pairwise sufficiently far apart, that the corresponding atoms do not overlap
                            in the reconstruction.
   :type inhibition_range: :class:`Union[int`, :class:`Tuple[int`, :class:`...]]`, *default* :class:`= None`
   :param backend: Defines the optimization backend.

                   * `'numpy'`: Selects the :class:`.NumPy_Backend`.
                   * `'numpy_fft'` (default): Selects the :class:`.NumPy_FFT_Backend`.
                   * `'numpy_caching_fft'`: Selects the :class:`.NumPy_CachingFFT_Backend`.
                   * `'pytorch'`: Selects the :class:`.PyTorch_Backend`.
                   * `'pytorch_fft'`: Selects the :class:`.PyTorch_FFT_Backend`.
   :type backend: ``{'numpy', 'numpy_fft', 'numpy_caching_fft', 'pytorch'}``, *default* = ``'numpy_fft'``
   :param logger: Logger instance used for intermediate output. If None, an internal logger instance will be created.
   :type logger: :class:`logging.Logger`, *default* :class:`= None`
   :param verbose: Verbosity level.

                   * 0: Show only errors.
                   * 1: Include warnings.
                   * 2: Include info.
                   * 3: Include debug messages.
   :type verbose: ``{0, 1, 2, 3}``, *default* :class:`= 0`
   :param \*\*kwargs: Keyword arguments that are handed to the constructor of the backend class.

   .. attribute:: W

      The dictionary tensor of shape ``(n_atoms, num_channels, *atom_shape)``.

      :type: :class:`np.ndarray`

   .. attribute:: H

      The activation tensor of shape ``(num_samples, num_atoms, *shift_shape)``.

      :type: :class:`np.ndarray`

   .. attribute:: R

      The reconstruction of the sample tensor using the current :attr:`W` and :attr:`H`.

      :type: :class:`np.ndarray`

   .. rubric:: Examples

   TODO: add examples

   .. rubric:: Notes

   Planned features:

       * Batch processing
       * Arbitrary transform types
       * Nested transformations
       * Additional reconstruction norms

   .. rubric:: References

   # TODO: add bibtex file

   .. [1] D.D. Lee, H.S. Seung, 2000. Algorithms for Non-negative Matrix Factorization,
       in: Proceedings of the 13th International Conference on Neural Information
       Processing Systems. pp. 535–541. https://doi.org/10.5555/3008751.3008829
   .. [2] R. Serizel, S. Essid, G. Richard, 2016. Mini-batch stochastic approaches for
       accelerated multiplicative updates in nonnegative matrix factorisation with
       beta-divergence, in: 26th International Workshop on Machine Learning for Signal
       Processing (MLSP). pp 1-6. http://ieeexplore.ieee.org/document/7738818/

   .. method:: W(self) -> numpy.ndarray
      :property:


   .. method:: H(self) -> numpy.ndarray
      :property:


   .. method:: V(self) -> numpy.ndarray
      :property:


   .. method:: R(self) -> numpy.ndarray
      :property:


   .. method:: R_partial(self, i_atom: int) -> numpy.ndarray


   .. method:: fit_batch(self, V: numpy.ndarray, n_iterations: int = 1000, update_H: bool = True, update_W: bool = True, keep_W: bool = False, sparsity_H: float = 0.0, inhibition_strength: float = 0.0, cross_atom_inhibition_strength: float = 0.0, progress_callback: Callable[[TransformInvariantNMF, int], bool] = None)

      Perform non-negative matrix factorization of samples :attr:`V`, i.e. optimization of dictionary :attr:`W` and
      activations :attr:`H`.

      :param V: Samples to be reconstructed. The shape of the sample tensor is ``(n_samples, n_channels, *sample_shape)``,
                where `sample_shape` is the shape of the individual samples and each sample consists of `n_channels`
                individual channels.
      :type V: :class:`np.ndarray`
      :param n_iterations: Maximum number of iterations (:attr:`W` and :attr:`H` updates) to be performed.
      :type n_iterations: :class:`int`, *default* :class:`= 1000`
      :param update_H: If False, the activation tensor :attr:`H` will not be updated.
      :type update_H: :class:`bool`, *default* :class:`= True`
      :param update_W: If False, the dictionary tensor :attr:`W` will not be updated.
      :type update_W: :class:`bool`, *default* :class:`= True`
      :param keep_W: If False, the dictionary tensor :attr:`W` will not be (re)initialized before starting iteration.
      :type keep_W: :class:`bool`, *default* :class:`= False`
      :param sparsity_H: Sparsity enforcing regularization for the :attr:`H` update.
      :type sparsity_H: :class:`float`, *default* :class:`= 0.`
      :param inhibition_strength: Lateral inhibition regularization factor for the :attr:`H` update within the same atom.
      :type inhibition_strength: :class:`float`, *default* :class:`= 0.`
      :param cross_atom_inhibition_strength: Lateral inhibition regularization factor for the :attr:`H` update across different atoms.
      :type cross_atom_inhibition_strength: :class:`float`, *default* :class:`= 0.`
      :param progress_callback: If provided, this function will be called after every iteration, i.e. after every update to :attr:`H` and
                                :attr:`W`. The first parameter to the function is the calling :class:`TransformInvariantNMF` instance, which can be
                                used to inspect intermediate results, etc. The second parameter is the current iteration step.

                                If the `progress_callback` function returns False, iteration will be aborted, which allows to implement
                                specialized convergence criteria.
      :type progress_callback: :class:`Callable[[```'TransformInvariantNMF'``, :class:`int]`, :class:`bool]`, *default* :class:`= None`


   .. method:: fit_minibatches(self, V: numpy.ndarray, algorithm: MiniBatchAlgorithm = MiniBatchAlgorithm.ASG_MU, batch_size: int = 3, n_epochs: int = 1000, sag_lambda: float = 0.2, keep_W: bool = False, sparsity_H: float = 0.0, inhibition_strength: float = 0.0, cross_atom_inhibition_strength: float = 0.0, progress_callback: Callable[[TransformInvariantNMF, int], bool] = None)

      Perform non-negative matrix factorization of samples :attr:`V`, i.e. optimization of dictionary :attr:`W` and
      activations :attr:`H` via mini-batch updates using a selection of algorithms from [3]_.

      :param V: Samples to be reconstructed. The shape of the sample tensor is ``(n_samples, n_channels, *sample_shape)``,
                where `sample_shape` is the shape of the individual samples and each sample consists of `n_channels`
                individual channels.
      :type V: :class:`np.ndarray`
      :param algorithm: MiniBatch update scheme to be used. See :class:`MiniBatchAlgorithm` and [3]_ for the different choices.
      :type algorithm: :class:`MiniBatchAlgorithm`
      :param batch_size: Number of samples per mini batch.
      :type batch_size: :class:`int`, *default* :class:`= 3`
      :param n_epochs: Maximum number of epochs across the full sample set to be performed.
      :type n_epochs: :class:`int`, *default* :class:`= 1000`
      :param sag_lambda: Exponential forgetting factor for for the stochastic _average_ gradient updates, i.e.
                         MiniBatchAlgorithm.ASAG_MU and MiniBatchAlgorithm.GSAG_MU
      :type sag_lambda: :class:`float`, *default* :class:`= 0.2`
      :param keep_W: If False, the dictionary tensor :attr:`W` will not be (re)initialized before starting iteration.
      :type keep_W: :class:`bool`, *default* :class:`= False`
      :param sparsity_H: Sparsity enforcing regularization for the :attr:`H` update.
      :type sparsity_H: :class:`float`, *default* :class:`= 0.`
      :param inhibition_strength: Lateral inhibition regularization factor for the :attr:`H` update within the same atom.
      :type inhibition_strength: :class:`float`, *default* :class:`= 0.`
      :param cross_atom_inhibition_strength: Lateral inhibition regularization factor for the :attr:`H` update across different atoms.
      :type cross_atom_inhibition_strength: :class:`float`, *default* :class:`= 0.`
      :param progress_callback: If provided, this function will be called after every (epoch-)iteration.
                                The first parameter to the function is the calling :class:`TransformInvariantNMF` instance, which can be
                                used to inspect intermediate results, etc. The second parameter is the current iteration step.

                                If the `progress_callback` function returns False, (epoch-)iteration will be aborted, which allows to implement
                                specialized convergence criteria.
      :type progress_callback: :class:`Callable[[```'TransformInvariantNMF'``, :class:`int]`, :class:`bool]`, *default* :class:`= None`

      .. rubric:: References

      .. [3] R. Serizel, S. Essid, G. Richard, 2016. Mini-batch stochastic approaches for
          accelerated multiplicative updates in nonnegative matrix factorisation with
          beta-divergence, in: 26th International Workshop on Machine Learning for Signal
          Processing (MLSP). pp 1-6. http://ieeexplore.ieee.org/document/7738818/


   .. method:: fit_stream(self, V: Iterator[numpy.ndarray], subsample_size: int = 3, max_subsamples: int = None, **kwargs)


   .. method:: fit(self, V: numpy.ndarray, **kwargs)



